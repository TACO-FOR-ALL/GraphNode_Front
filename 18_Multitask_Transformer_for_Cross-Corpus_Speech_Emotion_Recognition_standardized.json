[
  {
    "nodes": [
      {
        "name": "Multitask Transformer (cross-corpus / multitask variants)",
        "type": "Paper",
        "source_chunk_id": 1,
        "description": "IEEE Trans. Affective Computing, 2025. Proposes multitask transformer with contrastive learning and information maximization as auxiliary tasks."
      },
      {
        "name": "Cross-corpus Speech Emotion Recognition (SER / MER)",
        "type": "Problem",
        "source_chunk_id": 2,
        "description": "Generalization across disparate speech emotion datasets; low accuracy in cross-corpus scenarios."
      },
      {
        "name": "Cross-corpus Speech Emotion Recognition (SER / MER)",
        "type": "Problem",
        "source_chunk_id": 4,
        "description": "Extending cross-corpus SER to audio+text fusion with cross-corpus generalization challenges."
      },
      {
        "name": "Low cross-corpus generalization / low accuracy",
        "type": "Problem",
        "source_chunk_id": 1,
        "description": "Overall accuracy for cross-corpus SER remains relatively low and needs attention."
      },
      {
        "name": "Multitask Transformer (cross-corpus / multitask variants)",
        "type": "Method",
        "source_chunk_id": 4,
        "description": "Framework using a pre-trained transformer as backbone with SER as primary task and contrastive learning + information maximization as auxiliary tasks."
      },
      {
        "name": "Multitask Transformer (cross-corpus / multitask variants)",
        "type": "Method",
        "source_chunk_id": 8,
        "description": "Audio branch using a pre-trained Audio Spectrogram Transformer (AST) fine-tuned on AudioSet and then on SER."
      },
      {
        "name": "Multitask Transformer (cross-corpus / multitask variants)",
        "type": "Method",
        "source_chunk_id": 10,
        "description": "Text branch using pre-trained BERT with token-cutoff augmentation and analogous multitask losses."
      },
      {
        "name": "Auxiliary tasks / auxiliary losses",
        "type": "Method",
        "source_chunk_id": 9,
        "description": "Unsupervised auxiliary loss that attracts positive pairs (two augmentations of same instance) and repels negatives."
      },
      {
        "name": "Auxiliary tasks / auxiliary losses",
        "type": "Method",
        "source_chunk_id": 4,
        "description": "Unsupervised clustering-inspired loss that maximizes information (minimizes entropy) on logits to encourage clear class boundaries."
      },
      {
        "name": "Auxiliary tasks / auxiliary losses",
        "type": "Method",
        "source_chunk_id": 9,
        "description": "Auxiliary classifier predicting which of the audio augmentations was applied (five augmentation types)."
      },
      {
        "name": "Data Augmentation (audio and text)",
        "type": "Method",
        "source_chunk_id": 7,
        "description": "Audio: five waveform augmentation types (torch-audiomentations) applied twice per sample for contrastive learning. Text: token cutoff applied (five times)."
      },
      {
        "name": "Decision-level fusion",
        "type": "Method",
        "source_chunk_id": 10,
        "description": "During inference, logits from audio and text transformers are added for multimodal prediction (no fusion training)."
      },
      {
        "name": "IEMOCAP",
        "type": "Dataset",
        "source_chunk_id": 11,
        "description": "Approx. 12 hours dyadic English acted sessions; used with four emotion classes (neutral, happiness, sadness, anger)."
      },
      {
        "name": "MSP-IMPROV",
        "type": "Dataset",
        "source_chunk_id": 11,
        "description": "Dyadic English dataset with target sentences for emotions; used with four emotion classes; larger volume than IEMOCAP."
      },
      {
        "name": "EMO-DB (dataset + mentions/limitations)",
        "type": "Dataset",
        "source_chunk_id": 11,
        "description": "German acted dataset (800 utterances; used subset of 535 utterances mapped to four emotion classes)."
      },
      {
        "name": "MSP-PODCAST (dataset / challenge mentions)",
        "type": "Dataset",
        "source_chunk_id": 18,
        "description": "Naturalistic dataset (~238 hours); higher variability and distribution shift relative to lab datasets; used in preliminary experiments."
      },
      {
        "name": "Unweighted Average Recall (UAR)",
        "type": "Metric",
        "source_chunk_id": 12,
        "description": "Primary evaluation metric reported on target test set (Tte) for cross-corpus experiments."
      },
      {
        "name": "5% improvement over state-of-the-art (MSP-IMPROV to IEMOCAP)",
        "type": "Result",
        "source_chunk_id": 14,
        "description": "Reported significant improvement of 5% over prior state-of-the-art for MSP-IMPROV -> IEMOCAP cross-corpus setting."
      },
      {
        "name": "4% multimodal improvement (audio+text over unimodal)",
        "type": "Result",
        "source_chunk_id": 16,
        "description": "Adding text modality with decision-level fusion yields ~4% improvement over unimodal cross-corpus SER."
      },
      {
        "name": "2% decrease using ASR transcripts vs ground-truth",
        "type": "Result",
        "source_chunk_id": 16,
        "description": "When using ASR-produced transcripts instead of ground-truth text, multimodal accuracy decreased by ~2%."
      },
      {
        "name": "EMO-DB (dataset + mentions/limitations)",
        "type": "Result",
        "source_chunk_id": 14,
        "description": "Empirical observation that duplicating the small target EMO-DB dataset up to 8x improved cross-corpus performance (with increased variance)."
      },
      {
        "name": "Adversarial learning based multitask learning",
        "type": "Baseline",
        "source_chunk_id": 14,
        "description": "Prior common approach for cross-corpus SER using domain-adversarial objectives (gradient reversal / minimax)."
      },
      {
        "name": "Wav2Vec2",
        "type": "Baseline",
        "source_chunk_id": 17,
        "description": "Pre-trained waveform-based transformer (discussed/compared; authors favor AST over Wav2Vec2 for SER)."
      },
      {
        "name": "EMO-DB (dataset + mentions/limitations)",
        "type": "Limitation",
        "source_chunk_id": 11,
        "description": "EMO-DB is in German and transcripts are curated/neutral; thus not suitable for text-based emotion recognition in study."
      },
      {
        "name": "MSP-PODCAST (dataset / challenge mentions)",
        "type": "Limitation",
        "source_chunk_id": 18,
        "description": "MSP-PODCAST exhibits much larger variance between train and test; naturalistic conditions make cross-corpus transfer harder."
      },
      {
        "name": "Multimodal fusion generalization challenge",
        "type": "Limitation",
        "source_chunk_id": 4,
        "description": "Cross-corpus MER presents non-trivial fusion challenges due to higher specificity and doubled input feature sizes."
      }
    ],
    "edges": [
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "Multitask Transformer (cross-corpus / multitask variants)",
        "type": "proposes",
        "source_chunk_id": 4,
        "description": "Paper proposes this framework with contrastive learning and IM as auxiliary tasks.",
        "evidence": "1) We introduce a multitask transformer framework for cross-corpus Speech Emotion Recognition (SER), incorporating contrastive learning and information maximization (IM) as auxiliary tasks to enhance generalization.",
        "confidence": 0.95
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "Cross-corpus Speech Emotion Recognition (SER / MER)",
        "type": "addresses",
        "source_chunk_id": 4,
        "description": "Framework designed to improve cross-corpus generalization in SER.",
        "evidence": "We introduce a multitask transformer framework for cross-corpus Speech Emotion Recognition (SER)...",
        "confidence": 0.95
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "Cross-corpus Speech Emotion Recognition (SER / MER)",
        "type": "addresses",
        "source_chunk_id": 4,
        "description": "Framework extended to multimodal (audio+text) with decision-level fusion.",
        "evidence": "2) We extend the framework to multimodal emotion recognition (MER) with separate multitask transformers for audio and text, utilizing decision-level fusion during inference.",
        "confidence": 0.92
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "IEMOCAP",
        "type": "evaluates_on",
        "source_chunk_id": 1,
        "description": "Audio transformer evaluated in cross-corpus experiments using IEMOCAP as source or target.",
        "evidence": "We use publicly available and widely used speech corpora, including the IEMOCAP, MSP-IMPROV and EMO-DB databases.",
        "confidence": 0.9
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "MSP-IMPROV",
        "type": "evaluates_on",
        "source_chunk_id": 1,
        "description": "Audio transformer evaluated in cross-corpus experiments using MSP-IMPROV as source or target.",
        "evidence": "We use publicly available and widely used speech corpora, including the IEMOCAP, MSP-IMPROV and EMO-DB databases.",
        "confidence": 0.9
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "EMO-DB (dataset + mentions/limitations)",
        "type": "evaluates_on",
        "source_chunk_id": 1,
        "description": "Audio transformer evaluated in cross-corpus experiments using EMO-DB as source or target.",
        "evidence": "We use publicly available and widely used speech corpora, including the IEMOCAP, MSP-IMPROV and EMO-DB databases.",
        "confidence": 0.9
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "Auxiliary tasks / auxiliary losses",
        "type": "uses",
        "source_chunk_id": 4,
        "description": "Contrastive learning is employed as an unsupervised auxiliary task in the multitask framework.",
        "evidence": "In this paper, we leverage both contrastive learning and the IM loss, hypothesizing that these two methods can complement each other...",
        "confidence": 0.95
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "Auxiliary tasks / auxiliary losses",
        "type": "uses",
        "source_chunk_id": 4,
        "description": "IM loss used to encourage large-margin class boundaries without labels.",
        "evidence": "We leverage both contrastive learning and the IM loss... By combining the strengths of contrastive learning and IM loss, we learn rich knowledge of classi\ufb01cation boundary for both source and target datasets, without utilizing emotional labels from the target domain.",
        "confidence": 0.95
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "Data Augmentation (audio and text)",
        "type": "uses",
        "source_chunk_id": 7,
        "description": "Audio: five augmentation types (torch-audiomentations) to increase data by factor of five; Text: token cutoff augmentation.",
        "evidence": "We devised five types of augmentation ({ai,i =1 ... 5}) as presented in Table II. ... For the text, we employ the augmentation technique \u2018token cutoff\u2019 [50]... For the sake of consistency with the audio multitask transformer, the text augmentation was performed five times.",
        "confidence": 0.93
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "Decision-level fusion",
        "type": "uses",
        "source_chunk_id": 10,
        "description": "Multimodal MER achieved by decision-level addition of logits from audio and text branches at inference.",
        "evidence": "Decision-level fusion is performed by adding logits forward propagated from AST and BERT outputs.",
        "confidence": 0.92
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "5% improvement over state-of-the-art (MSP-IMPROV to IEMOCAP)",
        "type": "achieves",
        "source_chunk_id": 14,
        "description": "Reported significant improvement of ~5% over previous SOTA in one cross-corpus setting.",
        "evidence": "Likewise, our method exploits multitask learning and data augmentation but adopts transformer and contrastive learning to achieve significant improvement of 5% over the state-of-the-art.",
        "confidence": 0.88
      },
      {
        "start": "5% improvement over state-of-the-art (MSP-IMPROV to IEMOCAP)",
        "target": "Unweighted Average Recall (UAR)",
        "type": "measured_by",
        "source_chunk_id": 12,
        "description": "Reported improvements in cross-corpus experiments are quantified using UAR.",
        "evidence": "We record the unweighted average recall (UAR) on Tte.",
        "confidence": 0.95
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "Adversarial learning based multitask learning",
        "type": "outperforms",
        "source_chunk_id": 14,
        "description": "Authors report their unsupervised multitask approach outperforms adversarial learning approaches.",
        "evidence": "Our improved results indicate that unsupervised learning based multitask learning can outperform adversarial learning based multitask learning.",
        "confidence": 0.9
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "Wav2Vec2",
        "type": "outperforms",
        "source_chunk_id": 17,
        "description": "Authors validated AST choice over Wav2Vec2 and confirmed AST yielded significant improvement for SER.",
        "evidence": "We validated our choice of using AST over other transformer model like Wav2Vec2 and confirmed that switching to AST yielded significant improvement in the performance.",
        "confidence": 0.9
      },
      {
        "start": "Decision-level fusion",
        "target": "4% multimodal improvement (audio+text over unimodal)",
        "type": "achieves",
        "source_chunk_id": 16,
        "description": "Decision-level fusion produced approximately 4% improvement over unimodal cross-corpus SER.",
        "evidence": "Our experiment results show that our methods improve over unimodal cross-corpus SER; we obtained 4% improvement adding the text modality to cross-corpus SER.",
        "confidence": 0.9
      },
      {
        "start": "Decision-level fusion",
        "target": "2% decrease using ASR transcripts vs ground-truth",
        "type": "achieves",
        "source_chunk_id": 16,
        "description": "Using ASR transcripts at test time caused ~2% drop versus ground-truth transcripts.",
        "evidence": "Furthermore, our multimodal cross-corpus SER method has only a 2% decrease in accuracy when we used transcribed text instead of ground truth transcript, depicting the robustness of our method.",
        "confidence": 0.88
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "MSP-PODCAST (dataset / challenge mentions)",
        "type": "evaluates_on",
        "source_chunk_id": 18,
        "description": "Preliminary experiments conducted using IEMOCAP as source and MSP-PODCAST as test (naturalistic dataset).",
        "evidence": "We conducted preliminary experiments using the IEMOCAP dataset as the source and MSP-PODCAST as the test dataset, training a multitask transformer model that incorporates both modalities (refer to Table XIII).",
        "confidence": 0.9
      },
      {
        "start": "EMO-DB (dataset + mentions/limitations)",
        "target": "Multitask Transformer (cross-corpus / multitask variants)",
        "type": "suffers_from",
        "source_chunk_id": 11,
        "description": "EMO-DB cannot be used for text-based experiments due to language and curated transcripts.",
        "evidence": "Note that EMO-DB is in German while other datasets are in English. Its transcript were intentionally curated to be neutral. For these reasons, EMO-DB cannot be used for text-based emotion recognition experiments.",
        "confidence": 0.95
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "Multimodal fusion generalization challenge",
        "type": "suffers_from",
        "source_chunk_id": 4,
        "description": "Authors note cross-corpus MER introduces non-trivial fusion challenges and higher specificity.",
        "evidence": "However, cross-corpus MER introduces a non-trivial challenge for fusion with cross-corpus generalization. We acknowledge the complexity arising from the higher specificity of trained fusion models, with input feature sizes doubling compared to unimodal inputs.",
        "confidence": 0.9
      },
      {
        "start": "Auxiliary tasks / auxiliary losses",
        "target": "Data Augmentation (audio and text)",
        "type": "uses",
        "source_chunk_id": 7,
        "description": "Augmentation-type classifier uses labels from audio augmentations.",
        "evidence": "Augmentation labels are assigned according to augmentation type, and later used in an augmentation-type classi\ufb01er.",
        "confidence": 0.9
      }
    ]
  },
  {
    "nodes": [
      {
        "name": "Multitask Transformer (cross-corpus / multitask variants)",
        "type": "Paper",
        "source_chunk_id": 25,
        "description": "Paper title header (page 1591) indicating a multitask transformer approach for cross-corpus speech emotion recognition."
      },
      {
        "name": "Multitask Transformer (cross-corpus / multitask variants)",
        "type": "Method",
        "source_chunk_id": 25,
        "description": "Method named in the paper title: 'MULTITASK TRANSFORMER'."
      },
      {
        "name": "Cross-corpus Speech Emotion Recognition (SER / MER)",
        "type": "Problem",
        "source_chunk_id": 25,
        "description": "Problem stated in the paper title: generalization of speech emotion recognition across different corpora."
      }
    ],
    "edges": [
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "Multitask Transformer (cross-corpus / multitask variants)",
        "type": "proposes",
        "source_chunk_id": 25,
        "description": "Paper proposes the Multitask Transformer method (indicated by title).",
        "evidence": "MULTITASK TRANSFORMER FOR CROSS-CORPUS SPEECH EMOTION RECOGNITION",
        "confidence": 0.9
      },
      {
        "start": "Multitask Transformer (cross-corpus / multitask variants)",
        "target": "Cross-corpus Speech Emotion Recognition (SER / MER)",
        "type": "addresses",
        "source_chunk_id": 25,
        "description": "Method is intended to address cross-corpus speech emotion recognition (stated in title).",
        "evidence": "CROSS-CORPUS SPEECH EMOTION RECOGNITION",
        "confidence": 0.9
      }
    ]
  }
]